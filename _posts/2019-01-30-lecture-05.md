---
layout: distill
title: "Lecture 5: Parameter Estimation in Fully Observed Bayesian Networks"
description: Parameter estimation in graphical models.
date: 2019-01-30

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Chentao Ye # author's full name; let's order by sections
    url: "#"  # optional URL to the author's homepage
  - name: Muqiao Yang
    url: "#"
  - name: Qingtao Hu
    url: "#"
  - name: Sailun Xu
    url: "#"

editors:
  - name: Xun Zheng  # editor's full name
    url: "https://www.cs.cmu.edu/~xunzheng/"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction
In this previous lectures, we introduce the concept of **Graphical Models** and its mathematical formulations.
Now we know that we can use a graphical model $M$ (Bayesian network or undirected graph model) to specify a probability distribution $P_{M}$ satisfying some conditional independence property.
In this lecture, we will study how to utilize a graphical model. Given a GM $M$, we generally have two type of tasks
+ **Inference:** answering queries about the probability distribution $P_M$ defined by $M$, for examples, <script type="math/tex">P_M(X|Y)</script> where $X$ and $Y$ are subsets of variables in GM $M$.
+ **Learning:** estimating a plausible model $M$ from data $D$.  We call the process of obtaining a point estimate of $M$ as *learning*, but for Bayesian, they seek the posterior distribution of <script type="math/tex">p(M|D)</script>, which is actually an *inference* problem. The learning task is highly related to the inference task. When we want to compute a point estimate of $M$, we need to do inference to impute the missing data if not all the variables are observable. So the learning algorithm usually uses inference as a subroutine.

### Inference Problems
Here we will study different kind of queries associated with the probability distribution $P_M$ defined by GM $M$.

#### Likelihood
Most queries one may ask involve an *evidence*, so we first introduce the definition of evidence.
Evidence $\mathbf{e}$ is an assignment of a set of variables $\mathbf{E}$.
Without loss of generality, we assume that $\mathbf{E}=\{X_{k+1}, \cdots, X_k \}$.

The simplest kind of query is the probability of evidence $\mathbf{e}$

$$ P(\mathbf{e})=\sum_{x_1} \cdots \sum_{x_k} P(x_1, \cdots, x_k, \mathbf{e}) $$

this is often referred as computing the likelihood of $\mathbf{e}$.

#### Conditional Probability
We are often interested in the conditional probability of varaibles $X$ given evidence $\mathbf{e}$

$$ P(X|\mathbf{e}) = \frac{P(X,\mathbf{e})}{P(\mathbf{e})} = \frac{P(X,\mathbf{e})}{\sum_x P(X=x,\mathbf{e})} $$

this is the *a posteriori* belief $X$ given evidence $\mathbf{e}$. Usually we only query about a subset of variables $Y$ of all domain variables $X = {Y, Z}$ and "*don't care*" about the remaining, $Z$:

<script type="math/tex; mode=display"> P(Y | \mathbf{e}) = \sum_\mathbf{z} P(Y, Z = \mathbf{z} | \mathbf{e}).</script>

The process of summing out the "don't care" variables $Z$ is called *marginalization*,
and the resulting <script type="math/tex">P(Y | \mathbf{e})</script> is called a *marginal* prob.

A posteriori belief is very useful. Here we show some applications of a posteriori belief:
+ **Prediction:** computing the probability of an outcome given the starting condition.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/query-prediction.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example of prediction in a chain model.</strong> The green nodes are observable variables.
  </figcaption>
</figure>
In this type of queries, the query node is the descendent of the evidence. If we know the value of variable $A$ and $B$, the probability of the outcome is a posteriori belief <script type="math/tex">P(C|A,B)</script>. Using the conditional independence <script type="math/tex">C \perp A | B</script> encoded in the graph, we can simplify it to <script type="math/tex">P(C|A,B) = P(C|B)</script>.

+ **Diagnosis:** computing the probability of disease/fault given symptoms.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/query-diagnosis.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example of diagnosis in a chain model.</strong> The green nodes are observable variables.
  </figcaption>
</figure>
In this type of queries, the query node is the ancestor of the evidence.  In the GM $M$, if we know the value of variable $B$ and $C$, the probability of the cause is a posteriori belief <script type="math/tex">P(A|B,C)</script>. Again using the conditional independence, we can simplify it to <script type="math/tex">P(A|B,C) = P(A|B)</script>.
+ **Learning:** when learning with partial observation of the variables, we need to compute a posteriori belief in the learning algorithm. In EM algorithm, we will use a posteriori belief to fill in the unobserved variables as part of the algorithm. We will cover more details about learning algorithms later.

The information flow between variables is not restricted by the directionality of the edges in a GM.
We can actually do a probabilistic inference combing evidence from all parts of the networks.
Deep Belief Network (DBN) \[Hinton, 2006\]<d-cite key="hinton2006reducing"></d-cite> is an example.
DBN is a generative model or Restricted Boltzmann Machine (RBM) with multiple layers.
The model is successful for solving tasks like recognizing handwritten digits, learning motion capture data, collaborative filtering.
The following figures shows a DBN with 3 hidden layers. We can infer hidden unit $H_1, H_2, H_3$ from data $V$. We can also generate data $V$ by sampling hidden units $H_3, H_2, H_1$ in the opposite direction.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/dbn.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>A Deep Belief Network with 3 hidden layers for image processing. </strong>
  </figcaption>
</figure>

#### Most Probable Assignment
Another interesting query is to find the **most probable joint assignment**(MPA) for *some* variables in interest.
Such reasoning is usually performed under some evidence $\mathbf{e}$ and ignoring some "don't care" variables $Z$,

<script type="math/tex; mode=display"> \mathrm{MPA}(Y | e) = \arg \max_{\mathbf{y} \in \mathcal{Y} } P(\mathbf{y} | \mathbf{e}) = \arg \max_{\mathbf{y} \in \mathcal{Y} } P(\mathbf{y}, \mathbf{z} | \mathbf{e}). </script>

From the equation, we can find that MPA is the maximum a posteriori configuration of $Y$.

This query is typically useful for prediction given a GM $M$.
+ **Classification:** find the most likely label, given the evidence.
+ **Explanation** find the most likely scenario, given the evidence.

**Important Notice:** The MPA of a variable depends on its "context" of the problem --- the set of variables been jointly queried. For example, the probability distribution of $y_1$ and $y_2$ is shown in the following table. When we compute the MPA of $y_1$, we first compute the marginalization $p(y_1=0) = 0.4, p(y_1=1) = 0.6$, MPA is $\arg \max_{y_1} p(y_1) = 1$. On the other hand the MPA $y_1, y_2$ is $\arg \max_{y_1, y_2} p(y_1, y_2) = (0, 0)$.

| $y_1$ | $y_2$ | $p(y_1, y_2)$ |
|-------|-------|---------------|
| 0     | 0     | 0.35          |
| 0     | 1     | 0.05          |
| 1     | 0     | 0.3           |
| 1     | 1     | 0.3           |

### Inference Methods
Inference is generally a hard problem. Actually, there is a theorem showing that computing $P(X = \mathbf{x} | \mathbf{e})$ in a GM is NP-hard. However, the NP-hardness does not mean that we cannot solve inference.
The theorem implies that we cannot find a general inference procedure that works efficiently for arbitrary GMs.
We still have a chance to find provably efficient algorithms for some particular families of GMs.

There are many approaches for inference in GMs. They can be divided into two classes
+ **Exact inference algorithms.** Including the elimination algorithm, message-passing algorithm (sum-product, belief propagation), the junction tree algorithms. These algorithms can give the precise result of query. The major topic of this lecture is on exact inference algorithms.  
+ **Approximate inference techniques.** Including stochastic simulation / sampling methods, Markov chain Monte Carlo (MCMC) methods, variational algorithms. These algorithms only gives an approximate answer to the inference query. We will cover these methods in future lectures.

## Elimination Algorithm and Examples

Now that we understand the problem of inference, we will examine some simple cases to build intuition for a general method for exact inference.

### Elimination on Chains

Consider a simple chain on variables $A, B, C, D, E$ as seen below.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain1.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Chain PGM.</strong>
  </figcaption>
</figure>

Imagine we want the probability of $E=e$ regardless of the values of $A, B, C, D$. Naively, we could sum over the joint probability:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a, b, c, d, e)
$$

This will require an exponential number of terms. Thankfully, we can use the properties of Bayesian Networks to cut down on this computational cost. Since Bayesian Networks encode conditional independences, we can decompose the joint probability as follows:

$$
P(e) = \sum_d \sum_c \sum_b \sum_a P(a) P (b | a) P(c | b) P(d | c) P(e | d)
$$

This decomposition has allowed us to decouple conditionally independent variables and we can therefore push in and isolate summations, like the following:

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) \sum_a P(a) P (b | a)
$$

Focusing on the final term, $\sum_a P(a)P(b\vert a)$, we see that this marginalizes over $a$ and leaves us with a function of only $b$. We will generally refer to this as $\phi(b)$ but semantically it is equivalent to $P(b)$. We are left with the following expression for the marginal probability of $e$.

$$
P(e) = \sum_d \sum_c \sum_b P(c | b) P(d | c) P(e | d) P(b)
$$

Note that because the variable $a$ is no longer part of this expression we will say $a$ has been *eliminated*. We are therefore left with a new graphical model for our situation:

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/chain2.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Graphical Model after Elimination of A.</strong>
  </figcaption>
</figure>

Repeating this, we get the following sequence of steps:


<d-math block>
\begin{aligned}
P(e) &= \sum_d \sum_c P(d | c) P(e | d) \sum_b P(c | b) P(b) \\
&= \sum_d \sum_c P(d | c) P(e | d) P(c) \\
&= \sum_d P(e | d) \sum_c P(d | c) P(c) \\
&= \sum_d P(e | d) P(d) \\
\end{aligned}
</d-math>


As each elimination step costs $O(Val\vert X_i \vert \times \vert X_{i+1} \vert)$, the overall complexity is $O(nk^2)$, a huge improvement overall from the exponential runtime of the naive summation of the joint probability.

### Elimination in Hidden Markov Models

Now we will consider a model frequently used in time-series analysis and Natural Language Processing known as a Hidden Markov Model.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-04/HMM.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Hidden Markov Model.</strong>
  </figcaption>
</figure>

Naively we could find the conditional probability of $y_i$ given the observed sequence, but using our elimination trick, we can get similar computational advantages as seen in the chain example.


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-i}} P(y_1, \dots, y_T, x_1, \dots, x_T) \\
&= \sum_{y_{-i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(y_T | y_{T-1}) P(x_T | y_T) \\  
\end{aligned}
</d-math>


With this model, we have two intuitive choices for the order of variables to eliminate. We could start from the first time step (known as the *Forward Algorithm*) or start from the final time step (known as the *Backward Algorithm*).

Note that to each notation, we will represent a summation over all random variables $y$ except the $i$th variable as $y_{-i}$.

#### Forward Algorithm

If we choose to eliminate variables by starting at the beginning of the chain, we would first group factors as follows:


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) \sum_{y_1} P(y_1) P(x_1 | y_1) P(y_2 | y_1)\\  
&=  \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) \phi(x_1, y_2) \\  
&=  \sum_{y_{-1, -i}} P(x_2 | y_2) P(y_3 | y_2) \dots P(y_T | y_{T-1}) P(x_T | y_T) P(x_1, y_2) \\  
\end{aligned}
</d-math>

We can continue in this pattern with each intermediate term $\phi(\cdot)$ representing a joint probability.


#### Backward Algorithm

If we choose to eliminate variables by starting at the end of the chain, we would first group factors as follows:


<d-math block>
\begin{aligned}
P(y_i | x_1, \dots, x_T) &= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) \sum_{y_T} P(y_T | y_{T-1}) P(x_T | y_T) \\  
&= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) \phi(x_T, y_{T-1}) \\
&= \sum_{y_{-T, -i}} P(y_1) P(x_1 | y_1) P(y_2 | y_1) \dots P(x_{T-1} | y_{T-1}) P(y_{T-1} | y_{T-2}) P(x_T | y_{T-1}) \\
\end{aligned}
</d-math>

We can continue in this pattern with each intermediate term $\phi(\cdot)$ representing a conditional probability.

### Takeaways from Examples

The main takeaways from our exploration are that elimination provides a systematic way to efficiently do exact inference and that while we can generally create intermediate factors, the semantics of the intermediate factors can vary.

### Variable Elimination Algorithm

From these examples, we can consolidate our techniques used in the above examples to a more general algorithm called *Variable Elimination*.

Note that a frequent operation in the above examples is that of taking a product of factors ($F$) and then summing over the values of a variable. This is the general *Sum-Product* operation.

$$
\sum_{z} \prod_{\phi\in F} \phi
$$

Furthermore, we would like a way of incorporating evidence $E=e$ that generalizes our use of factors above. Because we will be maintaining a list of factors and iterating through it, it will be advantages to define new factors that explicitly incorporate our evidence, instead of having fixed variables inside our original factors.

Let $\delta(E_i, e_i)$ denote our *evidence potential*.

$$
\delta(E_i, e_i) = \{
1,\text{ if } E_i = e_i;
0,\text{ if } E_i \neq e_i
\}
$$

Then as fitting with out existing framework, we can simply define the *total evidence potential* to be the product our each of the individual evidence potentials.

$$
\delta(E, e) = \prod_{i \in I_e} \delta(E_i, e_i)
$$

Now we can treat evidence as just another type of factor.

With these concepts in hand we can outline our new algorithm.

* Given a query of the form $P(X_1 | e)$, we first focus on the joint probability $P(X_1, e)$.
$$P(X_1, e) = \sum_{x_n}\dots\sum_{x_2}\prod_{i} P(x_i | pa_i)$$.
This suggests an implicit "elimination order" over the variables.
* Following the order prescribed above:
  * Move all the relevant terms to the innermost sum and all irrelevant terms out of it.
  * Perform the Sum-Product operation on the innermost sum, producing a new factor $\phi$.
  * Repeat until the entire joint is calculated.
* So calculate the desired query, simply divide the joint by the marginal probability of the evidence.
$$
P(X_1 |e) = \frac{\phi(X_1, e)}{\sum_{x_1}\phi(X_1, e)}
$$

## Graph Elimination
In this section we are going to analyze the complexity of Variable Elimination (VE) algorithm. We first give a basic analysis based on the algorithm procedure and this can give us the insight of the bottleneck of complexity. Then we show that each step of VE can be viewed as a graph transformation step and this can let us analyze the algorithm complexity more clearly in graph perspective. We also formalize the graph perspective view of VA as graph elimination algorithm.

### Basic Complexity Analysis
From last section we have known VE can reduce inference complexity greatly. Now let’s have a closer look. Let $n$ be the number of variables in the fully joint probability, $m$ be the number of initial factors including original factors and evidence potentials. We have seen that VE is an n-step iterative elimination algorithm. In each step the algorithm picks a variable $X_i$ and “push in” the summarization w.r.t. $X_i$, which makes the summarization performs over the product of only a subset of factors involving $X_i$. Let $N_i$ be the number of variables ($y_1, y_2, …, y_{N_i}$) inside the subset of factors. We can formally write the step as:

$$
m_i(y_1,...,y_{N_i}) = \sum_{x_i \in Val(X_i)} \prod_{j=1}^{k_i} m(x_i, y_{c_i})
$$

Where $k_i$ is the number of factors involving $X_i$.

Assume each variable has no more than $v$ values, for each configuration of $y_1,...,y_{N_i}$ there is $\textnormal{Val}(X_i) k_i \le v N_i$ multiplication and $\textnormal{Val}(X_i) \le v$ addition. And there are at most $v^{N_i}$ configurations of $y_1,...,y_{N_i}$. Considering the algorithm has $n$ steps, and let $N_{max} = \max_i N_i$. We can write down the complexity is $O(nv^{N_{max}})$. Thus we see that the computational cost of VE algorithm is dominated by the maximum size of intermediate factors generated in an exponential growth rate.

### VE to Graph Elimination: Example
We have seen the bottleneck of the VE algorithm is the maximum size of intermediate factors. It is affected by the elimination ordering. Now let’s first see an example that connects iterative elimination steps inside VE with a series of graph structure transformations. This gives us a visualization way of analyzing complexity based on graph elimination. Questions regarding the computation complexity of the VE can be reduced to purely graph-theoretic considerations.

Given a Bayesian Network factorizing as the graph shown in below, we are going to do VE to inference $P(A | h)$. The initial factors are:

$$
P(a)P(b)P(c|b)P(d|a)P(e|c,d)P(f|a)P(g|e)P(h|e,f)
$$

Before doing VE we choose an elimination ordering as $H,G,F,E,D,C,B$.

<figure>
  <div class="row">
   <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example1.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

Step 1: to handle conditioning $h$

H variable node is observed node, we can add additional evidence indicator factor to make conditioning on observed evidence as isomorphic as a marginalization step:

$$
m_h(e,f) = p(h=\bar{h} |e, f) = \sum_{h} p(h|e,f) \delta(h=\bar{h})
$$

The new product of factors becomes:

$$
P(a)P(b)P(c|b)P(d|a)P(e|c,d)P(f|a)P(g|e)m_h(e,f)
$$

**Graph transformation:** After conditioning on $h$, we eliminate $h$ in old factors and generate a new factor $m_h(e, f)$. In the perspective of graph evolving, as shown in the below, this corresponding to delete the node $h$ as we eliminate $h$, and also add an edge between node $E$ and $F$ since the generated new factor depending on $E$ and $F$. The reduced factorization is a Gibbs distribution factorizing over the new graph.

<figure>
  <div class="row">
    <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example2.jpg' | relative_url }}" />
    </div>
  </div>
</figure>

Step 2: eliminate G

Compute:

$$
m_g(e) = \sum_g p(g|e) = 1
$$

The new product of factors:

$$
P(a)P(b)P(c|b)P(d|a)P(e|c,d)P(f|a)m_g(e)m_h(e, f)
$$

**Graph transformation:** Just remove node $G$ as the below.
<figure>
  <div class="row">
    <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example3.jpg' | relative_url }}" />
    </div>
  </div>
</figure>

Step 3: eliminate F

Compute:

$$
m_f(a, e) = \sum_f p(f|a)m_h(e, f)
$$
The new product of factors:

$$
P(a)P(b)P(c|b)P(d|a)P(e|c, d)m_g(e)m_f(a, e)
$$


**Graph transformation:** Remove node $F$ and moralize $A$ and $E$.
<figure>
  <div class="row">
   <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example4.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

Step 4: eliminate E

Compute:

$$
m_e(a,c,d) = \sum_e p(e|c, d)m_g(e)m_f(a, e)
$$

The new product of factors:

$$
P(a)P(b)P(c|b)P(d|a)m_e(a,c,d)
$$

**Graph transformation:** Generated term <=> fully connected subgraph, according to the Gibbs distribution property. As shown in the follwing, node $E$ is removed and $E$’s neighbors are moralized.
<figure>
  <div class="row">
   <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example5.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

Step 5: eliminate D

Compute:

$$
m_d(a, c) = \sum_d p(d|a) m_e(a, c, d)
$$

The new product of factors:

$$
P(a)P(b)P(c|b)m_d(a, c)
$$


**Graph transformation:** As in the following.
<figure>
  <div class="row">
   <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example6.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

Step 6: eliminate

Compute:

$$
m_c(a, b) = \sum_c p(c|b)m_d(a, c)
$$

The new product of factors:

$$
P(a)P(b)m_c(a, b)
$$


**Graph transformation:** As in the following, moralize $A$ and $B$.
<figure>
  <div class="row">
    <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example7.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

Step 7: eliminate B

Compute:

$$ m_b(a) = \sum_b p(b)m_c(alb)$$

The new product of factors:

$$ P(a)P(b)$$

**Graph transformation:**  Now only a single node $A$ left.

In the last step we just normalize left product.

All in all we can see the corresponding graph transformation can be shown as following. At each step we remove a node from the former graph and moralize removed node’s neighbors.
<figure>
  <div class="row">
   <div class="col three">
     <img src="{{ '/assets/img/notes/lecture-04/graph_example8.jpg' | relative_url }}" />
   </div>
  </div>
</figure>


### VE to Graph Elimination (GE): Formal Connection
As we have shown in the above example, intuitively the graph elimination procedure has a close connection with variable elimination algorithm. We first summarize the graph elimination algorithm, give out the definition of an important graph structure **reconstituted graph**, and a theorem about the correspondence of elimination clique in GE and the generated intermediate term in VE.

**Graph Elimination Algorithm** :

Given:  undirected/directed graph $G$, an elimination ordering $I$.    
Initialization: If $G$ is directed, first moralize $G$.   
Procedure: For each node $X_i$ in $I$, at each step connect all of the remaining neighbors of $X_i$ and remove $X_i$ from the graph.

**Reconstituted Graph $G’_I(V, E’)$**: also named induced graph   

Note: the $I$ is an elimination ordering, for different $I$ reconstituted graph is different.

Definition: the reconstituted graph $G’_I(V, E’)$ is a graph whose edge set $E’$ is a superset of $E$ containing all edges of $E$ and any new edges created during a run of **Graph Elimination Algorithm**.

Reconstituted graph records the **elimination cliques** created in graph elimination algorithm. At each step before we remove a node $X_i$ from graph, connecting all neighbors of $X_i$ created a clique, which is the **elimination cliques**

**Correspondence between intermediate terms in VE and elimination cliques in GE:**   

Following the corresponding steps of VE and GE, it’s easy to see that at each elimination step, the scope of generated intermediate term in VE is just he elimination clique generated in GE. The following figure shows this relationship based on the example we introduced before:
<figure>
  <div class="row">
   <div class="col three">
     <img src="{{ '/assets/img/notes/lecture-04/elimination_clique.jpg' | relative_url }}" />
   </div>
  </div>
</figure>

**Theorem:**  

1. The scope of every factor generated during the variable elimination process is a clique in reconstituted graph $G’_I(V, E’)$.

2. Every maximal clique in reconstituted graph $G’_I(V, E’)$ is the scope of some intermediate factor in the computation.

The proof of the theorem can be found in Chapter 9 of Koller’s PGM textbook. This theorem tells us the scope of intermediate factors which is a elimination clique, is a clique in reconstituted graph. What’s more, the scope of the largest intermediate factor is a the largest maximal clique in reconstituted graph.

### Complexity Analysis in Graph Perspective

In the beginning of this section we have argued that the bottleneck of VE’s complexity is determined by the scope size of the maximum intermediate factor generated in the procedure of VE. In above subsection we have shown that each intermediate factor in VE is an elimination clique in graph elimination algorithm, and the largest elimination clique is also a largest maximal clique in reconstituted graph.

Then, given an elimination ordering $I$, the problem of get the bottleneck maximum scope size $N_{max}$ of intermediate factors is equivalent of finding the largest clique in reconstituted graph $G’_I(V, E’)$, which is a pure graph theoretic question. And there are efficient mature algorithm for solving this problem.

### Elimination Ordering

We can define the **width** of a reconstituted graph as the size of largest clique minus 1. Let $w_{G,I}$ be the width of $G_I'(V, E’)$. For different ordering $I$, $w_{G,I}$ is different.

Now we define **tree-width** of $G$ as:

$$
w^*_{G} = \min_{I} w_{G,I}
$$

This term provides us a bound on the best performance	we can hope for applying VE to do an inference over a probability that factorizes over $G$.

However, finding the best elimination ordering of a graph is a NP-hard problem. As we have shown before the inference task itself is also NP-hard. But these two NP-hard problems are not same. To be more specific, even we have find a best elimination ordering, the complexity of inference can still be exponential if the tree width of the graph $G$ is large.

Although design a general best elimination ordering finding algorithm is NP-hard, there are some heuristic algorithm can generate a near-optimal elimination ordering (look at Koller’s PGM for detail). And on the other hand, for some particular graph $G$ there exists “obvious” optimal ordering that can be easily got. Now we give some examples to show opposite scenarios.

**Example 1: Star graph**  
If we remove centroid first it’s easy to see the width of induced graph is equal to $N-1$ where $N$ is total number of nodes. However if we remove centroid at the end, it’s easy to see the induced graph is the original star graph and the width is just 1. Using this elimination ordering can make variable elimination very efficient.

**Example 2: Tree graph**   
It’s obvious that eliminating nodes from leaves to root won’t introduce any induced dependency so the induced graph is just the original tree. And we know that there is no clique with size large than 3 in tree. So the width is just 1.

**Example 3: Ising model**  
It’s extremely hard to find a optimal elimination ordering. And actually the tree width of ising model is large than the $\sqrt{n}$ so even finding a optimal elimination ordering, the VE algorithm is still exponential in $n$.


### Bayesian Estimation for a Gaussian
* Reasons for Bayesian Approach
	* Update estimate sequentially over time
	* We may have prior knowledge about the expected magnitude of parameters
	* The MLE for $\Sigma$ may not be full rank if we don't have enough data

Now that we only consider conjugate priors, and consider various cases of increasing complexity:
* Known $\sigma$, unknown $\mu$
* Known $\mu$, unknown $\sigma$
* Unknown $\mu$ and $\sigma$

**Unknown $ \mu $, known $ \sigma $**
- Normal Prior:
	$$ P(\mu | \mu_0, \tau^2) \sim \mathcal{N}(\mu_0, \tau^2) = \frac{1}{\sqrt{2\pi\tau^2t}}\exp\{ -\frac{1}{2\tau^2}(\mu-\mu_0)^2 \}$$
- Joint Probability:
	$$ P(x, \mu | \mu_0, \tau^2) = P(x | \mu) P(\mu) = (\frac{1}{\sqrt{2\pi\sigma^2}})^N \exp\{ -\frac{1}{2\sigma^2}\sum_{n=1}^N(x_n - \mu)^2 \} \frac{1}{\sqrt{2\pi\tau^2}}\exp\{ -\frac{1}{2\tau^2}(\mu-\mu_0)^2 \} $$
- Posterior:
	$$ P(\mu | x, \mu_0, \tau^2) \sim \mathcal{N}(\tilde \mu, \tilde \sigma^2) = \frac{1}{\sqrt{2\pi \tilde \sigma^2}} \exp\{ - \frac{1}{2\tilde \sigma}(\mu-\tilde\mu)^2 \} $$
	
	$$\tilde \mu = \frac{N/\sigma^2}{N/\sigma^2 + 1/\tau^2} \bar{x} + \frac{1/\tau^2}{N/\sigma^2 + 1/\tau^2} \mu_0$$ 
	a convex combination of sample mean and prior mean, and 
	
	$$\tilde \sigma^{-2} = \frac{N}{\sigma}^2 + \frac{1}{\tau^2}$$
	is the precision of the prior plus $1/\sigma^2$ of contribution for each observed data point.
	
**Known $ \mu $, unkown $ \lambda = \sigma^{-2} $**
- Conjugate prior for $\sigma$:
	$$ P(\lambda | a, b)=  \Gamma(a)^{-1}b^a\lambda^{a-1}\exp(-b\lambda) $$
- Joint Probability:
	$$ P(x, \lambda | a, b) = P(x | \lambda, a, b)P(\lambda | a,b ) =  (2\pi)^{-n/2} \lambda^{n/2}\exp(-\frac{\lambda}{2}\sum_{n=1}^N(x_n - \mu)^2)  \Gamma(a)^{-1}b^a\lambda^{a-1}\exp(-b\lambda) $$

- Therefore Posterior:
	$$ P(\lambda | x, a, b) \sim Gamma(\lambda | a_n, b_n) = \Gamma(a_n)^{-1}b_n^{a_n}\lambda^{a_n - 1} \exp(-b_n \lambda)$$
	where $a_n = a + \frac{n}{2}$ and $b_n = b +\frac{1}{2}\sum_{n=1}^N(x_n - \mu)^2$

**Unkown $ \mu $, unkown $ \lambda $**
- Univariate Case
	- Conjugate prior for $\mu$ and $\lambda = \sigma^{-2}$: (Normal-Inverse-Gamma)
		$$ P(\mu, \sigma^2| m,V,a,b) = P(\mu | \sigma^2, m, V) P(sigma^2 | a,b) = \mathcal{\mu | m, \sigma^2 V} Gamma(\lambda | a_n, b_n)$$
- Multivariate Case
	- Conjugate prior for $\mu$ and $\Sigma$: (Normal-Inverse-Wishart)
		$$ P(\mu, \Sigma |\mu_0, \kappa_0, \Lambda_0^{-1}, \nu_0) = P(\mu | \Sigma, \mu_0, \kappa_0) P( \Sigma | \Lambda_0^{-1}, \nu_0) = \mathcal{N}(\mu | \mu_0, \frac{1}{\kappa_0}\Sigma) \mathcal{IW}(\Sigma |\Lambda_0^{-1}, \nu_0)  $$

### Two nodes fully observed BNs
<figure>
  <div class="row">
   <div class="col one">
     <img src="{{ '/assets/img/notes/lecture-05/two_node_bayesian.png' | relative_url }}" />
   </div>
  </div>
  <figcaption>
	<strong>Two nodes fully observed BNs</strong> 
  </figcaption>
</figure>

- Conditional Mixtures
	- Linear/Logistic Regression
- Classification
	- Generative and discriminative approaches 

### Classification
Goal: wish to learn $f: X \to Y$
- Generative: Modeling the joint distribution of all data, i.e.: $P(x,y)$
- Discriminative: Modeling only the conditional distribution, i.e.: $P(y \vert x)$

#### Conditional Gaussian
The data: $\{ (x_1,y_1),(x_2, y_2), \cdots, (x_n, y_n) \}$

$y$ is a class indicator vector (one-hot encoding):
$$ p(y_n) = multi(y_n; \pi) = \prod_{k=1}^K \pi_k^{y_{n,k}} $$

$x$ is a conditional Gaussian variable with a class-specific mean:
$$ p(x_n | y_{n,k} = 1, \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\{-\frac{1}{2\sigma^2}(x_n-\mu_k)^2 \}$$

$$p(x|y, \mu, \sigma)=\prod_n p(x_n | y_n, \mu, \sigma) = \prod_n (\prod_k \mathcal{N} (x_n; \mu_k, \sigma_k)^{y_{n,k}} ) $$

#### MLE of Conditional Gaussian
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-05/gaussian_mle.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>MLE of Conditional Gaussian.</strong> 
  </figcaption>
</figure>

- Data log-likelihood
	$$ l(\theta; D) = \log \prod_n p(x_n, y_n) = \log \prod_n p(y_n | \pi) p(x_n | y_n, \mu, \sigma) $$
- MLE:

	$$ \hat \pi_{k, MLE} = \text{argmax}_{\pi} l(\theta; D) \Rightarrow \hat \pi_{k, MLE} = \frac{\sum_{n}y_{n,k}}{N} = \frac{n_k}{N} $$, the fraction of sample of class $ k $
	
	$$ \hat \mu_{k, MLE} = \text{argmax}_{\mu} l(\theta; D) \Rightarrow \hat\mu_{k, MLE} = \frac{\sum_n y_{n,k} x_n}{\sum_n y_{n,k}} = \frac{\sum_{n} y_{n,k} x_n}{n_k} $$, the average of samples of class $ k $

#### Bayesian Estimation of Conditional Gaussian

- Prior:

	$$ P(\pi | \alpha) = Dirichlet(\pi; \alpha) $$         \\
	     $$ P(\mu_k | \nu) = \mathcal{N}(\mu_k; \nu, \tau^2) $$

- Posterior mean:

	$$ \pi_{k, Bayes} = \frac{N}{N + \vert a \vert_1} \hat \pi_{k, MLE} + \frac{\vert a \vert_1}{N + \vert a \vert_1} \frac{a_k}{ \vert a \vert_1} = \frac{n_k + a_k}{N + \vert a \vert_1} $$ where $ \vert a \vert_1 = \sum_{k=1}^K a_k $ 
	
	$$ \mu_{k, Bayes} = \frac{n_k/ \sigma^2}{n_k/\sigma^2 + 1/\tau^2}\hat{\mu}_{k, MLE} + \frac{1/\tau^2}{n_k/\sigma^2 + 1/\tau^2}\nu $$,
	$$ \sigma_{Bayes}^{-2} = \frac{N}{\sigma^2} + \frac{1}{\tau^2} $$ 

#### Gausian Distriminative Analysis:
- Joint probability of a datum and its label is:
	$$ P(x_n, y_{n,k} = 1 | \mu, \sigma ) = p(y_{n,k} = 1) p(x_n | y_{n,k} = 1, \mu, \sigma) = \pi_k \frac{1}{\sqrt{2\pi \sigma^2}}\exp\{-\frac{1}{2\sigma^2}(x_n - \mu_k)^2 \} $$

- Predict the conditional probability of label given the datum:
	$$ p(y_{n,k} = 1 | x_n, \mu, \sigma) = \frac{\pi_k (2\pi\sigma^2)^{-1/2} \exp \{ -(x_n - \mu_k)^2 /2\sigma^2 \} }{\sum_{k'} \pi_{k'} (2\pi\sigma^2)^{-1/2} \exp \{ -(x_n - \mu_{k'})^2 /2\sigma^2 \} } $$

- Frequentist Approach: fit $ \pi $, $ \mu $ and $ \sigma $ from data first
- Bayesian Approach: compute the posterior dist. of the parameters first

### Linear Regression

The data: $ \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N) \} $, where $ x $ is an input vector and $y$ is a response vector(either continuous or discrete)

A regression scheme can be used to model $ p(y \vert x) $ directly rather than $ p(x,y) $

#### A discrimintative probabilitstic model
Assume $ y_i = \theta^T x_i + \epsilon_i $, where $\epsilon$ is an error term of unmodeled effects or random noise. Assume $ \epsilon_i \sim \mathcal{N}(0, \sigma^2) $, then 
$$ p(y_i | x_i; \theta) \sim \mathcal{N}(\theta^Tx_i, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{ -\frac{1}{2\sigma^2} (y_i - \theta^Tx_i)^2 \} $$

And by that each $(y_i, x_i$ are i.i.d, we have:
$$ L(\theta) = \prod_{i=1}^N p(y_i | x_i; \theta) = (\frac{1}{\sqrt{2\pi\sigma^2}})^N \exp\{ - \frac{1}{2\sigma^2}\sum_{i=1}^N(y_i - \theta^T x_i)^2 \}$$

hence:
$$ l(\theta) = n\log \frac{1}{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2}\sum_{i=1}^N(y_i - \theta^Tx_i)^2$$

we notice that maximizing this term w.r.t $\theta$ is equivalent to minimizing a MSE(mean square error):
$$ J(\theta) = \frac{1}{2}\sum_{i=1}^N(x_i^T \theta - y_i)^2 $$

## ML Structure Learning for Completely Observed GMs
### Two "Optimal" Approaches
"Optimal" means the empoloyed algorithms guaranteed to return a structure that maximizes the objective. Some popluar heuristics, however, provide no gurantee on attaining optimality, interpretability or even do not have an explicit objective. e.g.: structured EM, Module network, greedy structural search, deep learning via auto-encoders, etc.

Will learn two classes of algorithms for guaranteed structure learning, albeit only applying to certain families of graphs:    
- Trees: The Chow-Liu Algorithm
- Pairwise MRFs: covariance selection, neighborhood-selection

### Structural Search
- $ O(2^{n^2}) $ graphs over $ n $ nodes
- $ O(n!) $ trees over $ n $ nodes
But we can find exact solution of an optimal tree under MLE:
- MLE score decomposable to edge-related elements
- In a tree each node has only one parent
Lead to the Chow-Liu Algorithm

### Chow-Liu Algorithm

<d-math block>
\begin{aligned} 
l(\theta_G, G; D) &= \log P(D | \theta_G, G)\\
&= \log \prod_n (\prod_i p(x_{n,i} | x_{n, \pi_i(G)}, \theta_{i | \pi_i(G)} ) )\\
&= \sum_{i} (\sum_{n}\log p(x_{n,i} | x_{n, \pi_i(G), \theta_{i | \pi_i(G) }}) )\\
&= M\sum_i (\sum_{x_i, x_{\pi_i(G)}} \hat p(x_i, x_{\pi_i(G)}\log p(x_i | x_{\pi_i(G)}, \theta_{i | \pi_i(G)} ))\\
&= M\sum_i(\sum_{x_i, x_{\pi_i(G)}} \hat{p}(x_i, x_{\pi_i(G)}) \log \frac{\hat p(x_i, x_{\pi_i(G)}, \theta_{i | \pi_i(G)})}{\hat p(x_{\pi_i(G)}) \hat p(x_i) } ) - M \sum_i (\sum_{x_i} \hat p(x_i) \log \hat p(x_i))\\
&= M\sum_i \hat I(x_i, x_{\pi_i(G)}) - M \sum_i \hat H(x_i)
\end{aligned} 
</d-math>

- For each pair of variable $x_i$ and $x_j$:
	- Compute empirical distribution: $ \hat p(x_i, x_j) = \frac{count(x_i, x_j)}{M} $
	- Compute mutual information: $ \hat I(x_i, x_j) = \sum_{x_i, x_j} \hat p (x_i, x_j) \log \frac{\hat p (x_i, x_j)}{\hat p (x_i) \hat p(x_j)} $
- Define a graph with node $ x_1, \cdots, x_n $:
	- Edge $ (i,j) $ get weight: $ \hat I(x_i, x_j) $
	- Compute maximum weight spanning tree
	- Direction: pick any node as root, do BFS do define directions

### Structure Learning for General Graphs
Theorem: The problem of learning a BN structure with at most $d$ parents is NP-hard for any (fixed) $ d \geq 2 $

Most structure learning approaches use heuristics 
- Exploit score decomposition:
	- e.g: Greedy search through space of node-orders
	- Local search of graph structures