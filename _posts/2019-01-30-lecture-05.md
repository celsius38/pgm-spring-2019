
---
layout: distill
title: "Lecture 5: Parameter Est. in fully observed BNs"
description: Introduction to the problem of Parameter Estimation in fully observed Bayesian Networks
date: 2019-02-10

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Chentao Ye  # author's full name; let's order by sections
    url: "#"  # optional URL to the author's homepage
  - name: Muqiao Yang
    url: "#"
  - name: Qingtao Hu
    url: "#"
  - name: Sailun Xu
    url: "#"

editors:
  - name: Xun Zheng  # editor's full name
    url: "https://www.cs.cmu.edu/~xunzheng/"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction
In this previous lectures, we introduce the concept of **Graphical Models** and its mathematical formulations.
Now we know that we can use a graphical model $M$ (Bayesian network or undirected graph model) to specify a probability distribution $P_{M}$ satisfying some conditional independence property.
In this lecture, we will study how to utilize a graphical model. Given a GM $M$, we generally have two type of tasks
+ **Inference:** answering queries about the probability distribution $P_M$ defined by $M$, for examples, <script type="math/tex">P_M(X|Y)</script> where $X$ and $Y$ are subsets of variables in GM $M$.
+ **Learning:** estimating a plausible model $M$ from data $D$.  We call the process of obtaining a point estimate of $M$ as *learning*, but for Bayesian, they seek the posterior distribution of <script type="math/tex">p(M|D)</script>, which is actually an *inference* problem. The learning task is highly related to the inference task. When we want to compute a point estimate of $M$, we need to do inference to impute the missing data if not all the variables are observable. So the learning algorithm usually uses inference as a subroutine.

### Inference Problems
Here we will study different kind of queries associated with the probability distribution $P_M$ defined by GM $M$.

#### Likelihood
Most queries one may ask involve an *evidence*, so we first introduce the definition of evidence.
Evidence $\mathbf{e}$ is an assignment of a set of variables $\mathbf{E}$.
Without loss of generality, we assume that $\mathbf{E}=\{X_{k+1}, \cdots, X_k \}$.

The simplest kind of query is the probability of evidence $\mathbf{e}$

$$ P(\mathbf{e})=\sum_{x_1} \cdots \sum_{x_k} P(x_1, \cdots, x_k, \mathbf{e}) $$

this is often referred as computing the likelihood of $\mathbf{e}$.

#### Conditional Probability
We are often interested in the conditional probability of varaibles $X$ given evidence $\mathbf{e}$

$$ P(X|\mathbf{e}) = \frac{P(X,\mathbf{e})}{P(\mathbf{e})} = \frac{P(X,\mathbf{e})}{\sum_x P(X=x,\mathbf{e})} $$

this is the *a posteriori* belief $X$ given evidence $\mathbf{e}$. Usually we only query about a subset of variables $Y$ of all domain variables $X = {Y, Z}$ and "*don't care*" about the remaining, $Z$:

<script type="math/tex; mode=display"> P(Y | \mathbf{e}) = \sum_\mathbf{z} P(Y, Z = \mathbf{z} | \mathbf{e}).</script>

The process of summing out the "don't care" variables $Z$ is called *marginalization*,
and the resulting <script type="math/tex">P(Y | \mathbf{e})</script> is called a *marginal* prob.

A posteriori belief is very useful. Here we show some applications of a posteriori belief:
+ **Prediction:** computing the probability of an outcome given the starting condition.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/query-prediction.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example of prediction in a chain model.</strong> The green nodes are observable variables.
  </figcaption>
</figure>
In this type of queries, the query node is the descendent of the evidence. If we know the value of variable $A$ and $B$, the probability of the outcome is a posteriori belief <script type="math/tex">P(C|A,B)</script>. Using the conditional independence <script type="math/tex">C \perp A | B</script> encoded in the graph, we can simplify it to <script type="math/tex">P(C|A,B) = P(C|B)</script>.

+ **Diagnosis:** computing the probability of disease/fault given symptoms.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/query-diagnosis.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example of diagnosis in a chain model.</strong> The green nodes are observable variables.
  </figcaption>
</figure>
In this type of queries, the query node is the ancestor of the evidence.  In the GM $M$, if we know the value of variable $B$ and $C$, the probability of the cause is a posteriori belief <script type="math/tex">P(A|B,C)</script>. Again using the conditional independence, we can simplify it to <script type="math/tex">P(A|B,C) = P(A|B)</script>.
+ **Learning:** when learning with partial observation of the variables, we need to compute a posteriori belief in the learning algorithm. In EM algorithm, we will use a posteriori belief to fill in the unobserved variables as part of the algorithm. We will cover more details about learning algorithms later.

The information flow between variables is not restricted by the directionality of the edges in a GM.
We can actually do a probabilistic inference combing evidence from all parts of the networks.
Deep Belief Network (DBN) \[Hinton, 2006\]<d-cite key="hinton2006reducing"></d-cite> is an example.
DBN is a generative model or Restricted Boltzmann Machine (RBM) with multiple layers.
The model is successful for solving tasks like recognizing handwritten digits, learning motion capture data, collaborative filtering.
The following figures shows a DBN with 3 hidden layers. We can infer hidden unit $H_1, H_2, H_3$ from data $V$. We can also generate data $V$ by sampling hidden units $H_3, H_2, H_1$ in the opposite direction.
<figure>
  <div class="row">
    <div class="col one">
      <img src="{{ '/assets/img/notes/lecture-04/dbn.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>A Deep Belief Network with 3 hidden layers for image processing. </strong>
  </figcaption>
</figure>

#### Most Probable Assignment
Another interesting query is to find the **most probable joint assignment**(MPA) for *some* variables in interest.
Such reasoning is usually performed under some evidence $\mathbf{e}$ and ignoring some "don't care" variables $Z$,

<script type="math/tex; mode=display"> \mathrm{MPA}(Y | e) = \arg \max_{\mathbf{y} \in \mathcal{Y} } P(\mathbf{y} | \mathbf{e}) = \arg \max_{\mathbf{y} \in \mathcal{Y} } P(\mathbf{y}, \mathbf{z} | \mathbf{e}). </script>

From the equation, we can find that MPA is the maximum a posteriori configuration of $Y$.

This query is typically useful for prediction given a GM $M$.
+ **Classification:** find the most likely label, given the evidence.
+ **Explanation** find the most likely scenario, given the evidence.

**Important Notice:** The MPA of a variable depends on its "context" of the problem --- the set of variables been jointly queried. For example, the probability distribution of $y_1$ and $y_2$ is shown in the following table. When we compute the MPA of $y_1$, we first compute the marginalization $p(y_1=0) = 0.4, p(y_1=1) = 0.6$, MPA is $\arg \max_{y_1} p(y_1) = 1$. On the other hand the MPA $y_1, y_2$ is $\arg \max_{y_1, y_2} p(y_1, y_2) = (0, 0)$.

| $y_1$ | $y_2$ | $p(y_1, y_2)$ |
|-------|-------|---------------|
| 0     | 0     | 0.35          |
| 0     | 1     | 0.05          |
| 1     | 0     | 0.3           |
| 1     | 1     | 0.3           |

### Inference Methods
Inference is generally a hard problem. Actually, there is a theorem showing that computing $P(X = \mathbf{x} | \mathbf{e})$ in a GM is NP-hard. However, the NP-hardness does not mean that we cannot solve inference.
The theorem implies that we cannot find a general inference procedure that works efficiently for arbitrary GMs.
We still have a chance to find provably efficient algorithms for some particular families of GMs.

There are many approaches for inference in GMs. They can be divided into two classes
+ **Exact inference algorithms.** Including the elimination algorithm, message-passing algorithm (sum-product, belief propagation), the junction tree algorithms. These algorithms can give the precise result of query. The major topic of this lecture is on exact inference algorithms.  
+ **Approximate inference techniques.** Including stochastic simulation / sampling methods, Markov chain Monte Carlo (MCMC) methods, variational algorithms. These algorithms only gives an approximate answer to the inference query. We will cover these methods in future lectures.

## Generalized Linear Models

Generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows the linear model to be related to response variables via a link function, that have error distribution models other than a normal distribution. For example both linear regression and logistic regression can be unified by generalized linear model.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-05/GLM.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>GLM</strong>
  </figcaption>
</figure>

### Commonality

$E_p(\mathbf{y)} = \mu = f(\theta^T\mathbf{x})$ 

where $f$ is the response function.

The observed input $\mathbf{x}$ is assumed to enter into the model via a linear combination of its elements, and the conditional mean $\mu$ is represented as a function $f(\xi)$ of $\xi$, where f is known as the link function of $\xi=\theta^T\mathbf{x}$. The observed output $\mathbf{y}$ is assumed to be characterized by an exponential family distribution with conditional mean $\mu$.

### MLE for GLIMs

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-05/responsFunc.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example canonical response functions</strong>
  </figcaption>
</figure>

Log-likelihood $$\ell = \sum\limits_nlogh(y_n)+\sum\limits_n(\theta^Tx_ny_n-A(\eta_n))$$
Derivative is $$\frac{d\ell}{dt}=\sum_n(x_ny_n-\frac{dA(\eta_n)}{\eta_n}\frac{d\eta}{d\theta})=\sum_nx_n(y_n-\mu_n)=X^T(y-\mu)$$
which is a fixed point function since $\mu$ is a function of $\theta$.

 ### Iteratively Reweighted Least Squares (IRLS)

Recall that the Hessian matrix $H=-X^TWX$, and $\theta^*=(X^TX)^{-1}X^Ty$ in least mean square optimization, we use Newton-Raphson method with cost function $\ell$ $$\theta^{t+1}=\theta^t-H^{-1}\nabla_\theta\ell=(X^TW^tX)^{-1}X^TW^tz^t$$
where the response is $z^t=X\theta^t+(W^t)^{-1}(y-\mu^t)$. Hence, this can be understood as solving the Iteratively reweighted least squares problem
$$\theta^{t+1}=arg\min_\theta(z-X\theta)^T(z-X\theta)$$

## Global and local parameter independence

Simple graphical models can be viewed as building blocks of complex graphical models. With the same concept, if we assume the parameters for each local conditional probabilistic distribution to be globally independent, and all nodes are fully observed, then the log-likelihood function can be decomposed into a sum of local terms, one per node
$$\ell(\theta,D)=log p(D|\theta)=\sum_i(\sum_nlogp(x_{n,i}|\mathbf{x_{n,\pi_i},\theta_i}))$$

### Plate

A plate is a macro that allows subgraphs to be replicated. Conventionally, instead of drawing each repeated variable individually, a plate is used to group these variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-05/plate.png' | relative_url }}" />
    </div>
  </div>
</figure>

Rules for plates: Repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. $N$), updating the plate index variable (e.g. $n$) as you go; Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure.

For example, in the directed acyclic network, it can be decomposed as 
$$p(x|\theta)=\sum_{i=1}^np(x_i|\mathbf{x}_{\pi_i})=p(x_1|\theta_1)p(x_2|x_1,\theta_2)p(x_3|x_1,\theta_3)p(x_4|x_2,x_3,\theta_4)$$

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-05/decompose.png' | relative_url }}" />
    </div>
  </div>
</figure>

Global parameter independence:
For every DAG model,  $p(\theta_m|G)=\sum_{i=1}\limits^Mp(\theta_i|G)$

Local parameter independence:
For every node, $p(\theta_i|G)=\sum\limits_{i=1}^{q_i}p(\theta_{x_i^k|\mathbf{x}_{\pi_i}^j}|G)$

### Global parameter sharing
Consider a network structure $G$ over a set of variables $X = \{X_1,...,X_n\}$, parameterized by a set of parameters $θ$. Each variable $X_i$ is associated with a CPD $P(X_i | U_i,\theta)$. Now, rather than assume that each such CPD has its own parameterization $\theta_{X_i|U_i}$, we assume that we have a certain set of shared parameters that are used by multiple variables in the network. That is the global parameter sharing. Assuming $\theta$ is partitioned into disjoint subsets $\theta_1,...,\theta_k$, and with each subset we assign a disjoint set of variables $\mathcal{V}_k\subset\mathcal{X}$. For $X_i\subset\mathcal{V_k}$, $$P(X_i|\mathbf{U}_i,\theta)=P(X_i|\mathbf{U}_i,\theta^k)$$
and for $X_i,X_j\subset\mathcal{V_k}$, $$P(X_i|\mathbf{U}_i,\theta^k)=P(X_j|\mathbf{U}_j,\theta^k)$$